{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7155750,"sourceType":"datasetVersion","datasetId":4132397},{"sourceId":7156637,"sourceType":"datasetVersion","datasetId":4133083}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nimport os\nfrom PIL import Image\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nimport albumentations as A\nfrom torch.utils.data import Dataset, DataLoader\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\nfrom torchvision import transforms, models\nfrom efficientnet_pytorch import EfficientNet\nimport warnings\nfrom torchvision.models import resnet, densenet\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-10T17:08:44.547159Z","iopub.execute_input":"2023-12-10T17:08:44.547930Z","iopub.status.idle":"2023-12-10T17:08:44.555966Z","shell.execute_reply.started":"2023-12-10T17:08:44.547896Z","shell.execute_reply":"2023-12-10T17:08:44.554829Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"#!pip install efficientnet_pytorch","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:08:44.557761Z","iopub.execute_input":"2023-12-10T17:08:44.558086Z","iopub.status.idle":"2023-12-10T17:08:44.568721Z","shell.execute_reply.started":"2023-12-10T17:08:44.558033Z","shell.execute_reply":"2023-12-10T17:08:44.567866Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# Importation des donnees d'apprentissage\ndata = pd.read_csv('/kaggle/input/dossier/Train.csv')\ndf = data.copy()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:08:44.746205Z","iopub.execute_input":"2023-12-10T17:08:44.746557Z","iopub.status.idle":"2023-12-10T17:08:44.765162Z","shell.execute_reply.started":"2023-12-10T17:08:44.746525Z","shell.execute_reply":"2023-12-10T17:08:44.764254Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"                Image_id  Label\n0      id_004wknd7qd.jpg  blast\n1  id_004wknd7qd_rgn.jpg  blast\n2      id_005sitfgr2.jpg  brown\n3  id_005sitfgr2_rgn.jpg  brown\n4      id_00stp9t6m6.jpg  blast","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_id</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_004wknd7qd.jpg</td>\n      <td>blast</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_004wknd7qd_rgn.jpg</td>\n      <td>blast</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_005sitfgr2.jpg</td>\n      <td>brown</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_005sitfgr2_rgn.jpg</td>\n      <td>brown</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_00stp9t6m6.jpg</td>\n      <td>blast</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['Label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:08:44.766811Z","iopub.execute_input":"2023-12-10T17:08:44.767196Z","iopub.status.idle":"2023-12-10T17:08:44.775886Z","shell.execute_reply.started":"2023-12-10T17:08:44.767166Z","shell.execute_reply":"2023-12-10T17:08:44.774908Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"Label\nblast      2988\nbrown      1532\nhealthy     820\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"on peut dire que la classe \"blast\" est sur-représentée par rapport aux classes \"brown\" et \"healthy\". La classe \"blast\" a 2988 instances, tandis que \"brown\" a 1532 instances et \"healthy\" a 820 instances.\n\nLa sur-représentation d'une classe peut avoir des implications lors de l'entraînement d'un modèle, en particulier dans des tâches d'apprentissage automatique supervisé. Un modèle peut être biaisé vers la classe sur-représentée et peut ne pas généraliser correctement aux classes sous-représentées. On va essayé de corriger cela en en faisant une validation croisée tout en imposant une certaine proportion de chaque classe dans mes différents blocs.","metadata":{}},{"cell_type":"code","source":"# Exploration des données\nplt.figure(figsize=(10, 5))\ndf['Label'].value_counts().plot(kind='bar', title='Distribution des classes')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:08:44.777568Z","iopub.execute_input":"2023-12-10T17:08:44.777907Z","iopub.status.idle":"2023-12-10T17:08:45.050493Z","shell.execute_reply.started":"2023-12-10T17:08:44.777882Z","shell.execute_reply":"2023-12-10T17:08:45.049439Z"},"trusted":true},"execution_count":59,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x500 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# On va encoder nos variables et aussi faire une validation croisée en s'assurant que dans chaque bloc un certtain pourcentage de toutes les classes soient représentés\n\n# Initialisation pour l'encodage \nencoder = LabelEncoder()\ndf['score'] = encoder.fit_transform(df['Label'])\ndf = df.reset_index()\n\n\n# Spécifiez les proportions de classe désirées pour chaque bloc\nproportions_classes = {'blast': 0.4, 'brown': 0.4, 'healthy': 0.2}\n\n# Créer StratifiedKFold avec des proportions de classe personnalisées\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Créer une nouvelle colonne pour les blocs et l'initialiser à -1\ndf['fold'] = -1\n\nfor n, (train_index, valid_index) in enumerate(kf.split(df, df['score'])):\n    # Obtenir la distribution des classes dans l'ensemble d'entraînement\n    distribution_classes_entrainement = df.iloc[train_index]['score'].value_counts(normalize=True)\n\n    # Initialiser la colonne de bloc à -1\n    df.loc[valid_index, 'fold'] = n\n\n    # Ajuster les affectations de bloc pour respecter les proportions de classe désirées\n    for label, proportion in proportions_classes.items():\n        # Calculer le nombre d'instances nécessaires pour la classe actuelle\n        num_instances = int(proportion * len(valid_index))\n        # Obtenir le nombre réel d'instances dans l'ensemble d'entraînement\n        num_instances_reel = int(distribution_classes_entrainement[encoder.transform([label])[0]] * len(train_index))\n\n        # Si le nombre réel d'instances est inférieur à celui requis, déplacer des instances du jeu de validation vers le jeu d'entraînement\n        if num_instances_reel < num_instances:\n            # Obtenir les indices des instances avec l'étiquette actuelle dans le jeu de validation\n            indices_label = df.loc[valid_index][df['score'] == encoder.transform([label])[0]].index\n            # Déplacer les instances excédentaires du jeu de validation vers le jeu d'entraînement\n            df.loc[indices_label[:instances_excédentaires], 'fold'] = n - 1\n\n# Vérifier la distribution des classes dans chaque bloc\nfor bloc in range(5):\n    print(f\"Bloc {bloc}:\")\n    print(df[df['fold'] == bloc]['score'].value_counts(normalize=True))\n    print(\"=\"*30)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:08:45.052216Z","iopub.execute_input":"2023-12-10T17:08:45.052513Z","iopub.status.idle":"2023-12-10T17:08:45.090396Z","shell.execute_reply.started":"2023-12-10T17:08:45.052487Z","shell.execute_reply":"2023-12-10T17:08:45.089431Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Bloc 0:\nscore\n0    0.559925\n1    0.286517\n2    0.153558\nName: proportion, dtype: float64\n==============================\nBloc 1:\nscore\n0    0.559925\n1    0.286517\n2    0.153558\nName: proportion, dtype: float64\n==============================\nBloc 2:\nscore\n0    0.559925\n1    0.286517\n2    0.153558\nName: proportion, dtype: float64\n==============================\nBloc 3:\nscore\n0    0.558989\n1    0.287453\n2    0.153558\nName: proportion, dtype: float64\n==============================\nBloc 4:\nscore\n0    0.558989\n1    0.287453\n2    0.153558\nName: proportion, dtype: float64\n==============================\n","output_type":"stream"}]},{"cell_type":"code","source":"# df.to_csv(r'folds.csv')","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:08:45.091585Z","iopub.execute_input":"2023-12-10T17:08:45.091899Z","iopub.status.idle":"2023-12-10T17:08:45.096128Z","shell.execute_reply.started":"2023-12-10T17:08:45.091872Z","shell.execute_reply":"2023-12-10T17:08:45.095053Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# Vérifions si les images dans dont les Id sont dans le jeu de donnees Test et Train sont bien dans le repertoire image \n\ndef verifier_correspondances(repertoire_images, chemin_fichier_csv):\n\n    df = pd.read_csv(chemin_fichier_csv)\n\n    # Liste des noms de fichiers dans le répertoire d'images\n    noms_fichiers_images = set(os.listdir(repertoire_images))\n\n    # Liste des noms de fichiers dans le CSV\n    noms_fichiers_csv = set(df['Image_id'].tolist())\n\n    # Vérifier si tous les noms de fichiers dans le CSV sont présents dans le répertoire d'images\n    noms_manquants = noms_fichiers_csv - noms_fichiers_images\n\n    if not noms_manquants:\n        print(f\"Tous les noms de fichiers du CSV sont présents dans le répertoire d'images.\")\n    else:\n        print(f\"Certains noms de fichiers du CSV ne sont pas présents dans le répertoire d'images:\")\n        print(noms_manquants)\n\n# les chémins \nrepertoire_images = '/kaggle/input/hussen1/Images'\nchemin_fichier_train = '/kaggle/input/dossier/Train.csv'\nchemin_fichier_test = '/kaggle/input/dossier/Test.csv'\n\n# Vérifions pour le Jeu de donnees Train\nverifier_correspondances(repertoire_images, chemin_fichier_train)\n\n# Vérifions pour le jeu de donnees Test \nverifier_correspondances(repertoire_images, chemin_fichier_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:08:45.099614Z","iopub.execute_input":"2023-12-10T17:08:45.100348Z","iopub.status.idle":"2023-12-10T17:08:45.129102Z","shell.execute_reply.started":"2023-12-10T17:08:45.100310Z","shell.execute_reply":"2023-12-10T17:08:45.128284Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Tous les noms de fichiers du CSV sont présents dans le répertoire d'images.\nTous les noms de fichiers du CSV sont présents dans le répertoire d'images.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Fonction pour fixer les graines aléatoires dans différents modules\n\ndef set_everything(seed=42):\n    \n    # Fixer la graine du générateur de nombres pseudo-aléatoires de Python\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \n    # Fixer la graine du générateur de nombres aléatoires de NumPy\n    np.random.seed(seed)\n    \n    # Fixer la graine du générateur de nombres aléatoires de PyTorch (CPU)\n    torch.manual_seed(seed)\n    \n    # Fixer la graine du générateur de nombres aléatoires de PyTorch\n    torch.cuda.manual_seed(seed)\n    \n    # Assurer la déterministicité des opérations de convolution de PyTorch sur GPU (si CUDA est disponible)\n    torch.backends.cudnn.deterministic = True\n\n# Appeler la fonction avec une graine par défaut de 42\nset_everything()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:08:45.130344Z","iopub.execute_input":"2023-12-10T17:08:45.130999Z","iopub.status.idle":"2023-12-10T17:08:45.137427Z","shell.execute_reply.started":"2023-12-10T17:08:45.130964Z","shell.execute_reply":"2023-12-10T17:08:45.136277Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# Définission d'une classe personnalisée Dataset en utilisant la classe Dataset de PyTorch.\n\nclass Dataset(Dataset):\n    def __init__(self, image, file, mode, fold, h, w, transform=None):\n        self.image = image\n        self.w = w\n        self.h = h\n        self.transform = transform\n        if mode == 'train':\n            self.file = pd.read_csv(file)[pd.read_csv(file)['fold'] != fold].reset_index(drop=True)\n        else:\n            self.file = pd.read_csv(file)[pd.read_csv(file)['fold'] == fold].reset_index(drop=True)\n\n    def __len__(self):\n        return self.file.shape[0]\n\n    def __getitem__(self, index):\n        img_id = str(self.file['Image_id'][index])\n\n        # Vérifier si 'Image_id' est une valeur valide\n        if pd.notna(img_id):\n            img_path = os.path.join(self.image, img_id)\n\n            # Vérifier si le fichier existe avant de tenter de l'ouvrir\n            if os.path.exists(img_path):\n                image = np.array(Image.open(img_path).convert('RGB'))\n            else:\n                # Gérer le cas où le fichier n'existe pas\n                image = np.zeros((self.h, self.w, 3), dtype=np.uint8)\n\n            if self.transform is not None:\n                augmentations = self.transform(image=image)\n                image = augmentations['image']\n\n            y = self.file['score'][index]\n            if y == 1:\n                y = [1, 0, 0]\n            elif y == 2:\n                y = [0, 1, 0]\n            else:\n                y = [0, 0, 1]\n\n            return image, torch.tensor(y)\n\n        else:\n            # Gérer le cas où 'Image_id' est manquant ou égal à NaN\n            image = np.zeros((self.h, self.w, 3), dtype=np.uint8)\n            y = [0, 0, 0]\n\n            return image, torch.tensor(y)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:08:45.139006Z","iopub.execute_input":"2023-12-10T17:08:45.139648Z","iopub.status.idle":"2023-12-10T17:08:45.153490Z","shell.execute_reply.started":"2023-12-10T17:08:45.139621Z","shell.execute_reply":"2023-12-10T17:08:45.152539Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# Taux d'apprentissage (learning rate) pour l'optimiseur\nLR = 1e-4\n\n# Taille du lot d'entraînement (batch size)\nBS = 32\n\n# Nombre d'époques d'entraînement\nNE = 5 \n# Hauteur des images d'entrée\nH = 192\n\n# Largeur des images d'entrée\nW = 224\n\n# Chemin vers le répertoire contenant les images\nIMG = r'/kaggle/input/hussen1/Images'\n\n# Chemin vers le fichier CSV contenant les informations sur les plis (folds)\nFILE = r'/kaggle/input/dossier/folds.csv'\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:08:45.154517Z","iopub.execute_input":"2023-12-10T17:08:45.154848Z","iopub.status.idle":"2023-12-10T17:08:45.167693Z","shell.execute_reply.started":"2023-12-10T17:08:45.154821Z","shell.execute_reply":"2023-12-10T17:08:45.166835Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"def get_loaders(image, file, fold, h, w, bs, train_transform, val_transform, pin_memory=True):\n    \"\"\"\n    Fonction pour obtenir les loaders d'entraînement et de validation.\n\n    Parameters:\n        image (str): Chemin vers le dossier contenant les images.\n        file (str): Chemin vers le fichier CSV contenant les informations sur les images.\n        fold (int): Numéro du pli pour la validation croisée.\n        h (int): Hauteur des images après redimensionnement.\n        w (int): Largeur des images après redimensionnement.\n        bs (int): Taille du lot (batch size).\n        train_transform (albumentations.Compose): Transformations d'augmentation pour l'entraînement.\n        val_transform (albumentations.Compose): Transformations pour la validation.\n        pin_memory (bool): Indique si l'utilisation de la mémoire GPU doit être activée.\n\n    Returns:\n        DataLoader: Loader d'entraînement.\n        DataLoader: Loader de validation.\n    \"\"\"\n    # Crée des instances de Dataset pour les ensembles d'entraînement et de validation\n    train_ds = Dataset(image=image, file=file, mode='train', fold=fold, w=w, h=h, transform=train_transform)\n    val_ds = Dataset(image=image, file=file, mode='val', fold=fold, w=w, h=h, transform=val_transform)\n\n    # Crée des loaders d'entraînement et de validation\n    train_loader = DataLoader(train_ds, batch_size=bs, pin_memory=pin_memory, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=bs, pin_memory=pin_memory, shuffle=False)\n\n    return train_loader, val_loader\n\n# Transformations d'augmentation de données\nnormalize = A.Normalize(\n    mean=[0.485, 0.456, 0.406],\n    std=[0.229, 0.224, 0.255],\n    max_pixel_value=255.0\n)\n\ntrain_transform = A.Compose([\n    A.Resize(width=W, height=H),\n    A.HorizontalFlip(p=0.6),\n    A.VerticalFlip(p=0.4),\n    A.Rotate(limit=40),\n    normalize,\n    ToTensorV2()\n], is_check_shapes=False)\n\nval_transform = A.Compose([\n    A.Resize(width=W, height=H),\n    normalize,\n    ToTensorV2()\n], is_check_shapes=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:08:45.169030Z","iopub.execute_input":"2023-12-10T17:08:45.169282Z","iopub.status.idle":"2023-12-10T17:08:45.183002Z","shell.execute_reply.started":"2023-12-10T17:08:45.169260Z","shell.execute_reply":"2023-12-10T17:08:45.182137Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"Dans la suite on va effectuer une comparaison entre trois modèles sur nos données : EfficientNet, ResNetModel, DenseNetModel.\n    \n1. **EfficientNet** :\n\n* Caractéristique principale : Conçu pour optimiser la performance et l'efficacité en ajustant simultanément la largeur, la profondeur, et la résolution des couches du réseau.\n\n* Évolutivité : Les modèles EfficientNet sont évolués en utilisant un paramètre de mise à l'échelle (compound scaling) qui garantit un équilibre entre la largeur, la profondeur et la résolution du réseau.\n\n2. **ResNet (Residual Networks)** :\n\n* Caractéristique principale : Introduit des connexions résiduelles qui permettent un flux direct d'informations à travers les couches du réseau, facilitant l'apprentissage profond en évitant les problèmes de disparition du gradient.\n\n* Architecture résiduelle : Les blocs résiduels, ou \"residual blocks\", contiennent des connexions skip (shortcuts) qui bypassent une ou plusieurs couches, facilitant l'apprentissage profond.\n3. **DenseNet (Densely Connected Convolutional Networks)** :\n\n* Caractéristique principale : Propose une architecture dense dans laquelle chaque couche reçoit des informations de toutes les couches précédentes dans un bloc.\n\n* Connectivité dense : Les blocs dans DenseNet sont reliés de manière dense, ce qui signifie que chaque couche reçoit des entrées non seulement de la couche précédente mais aussi de toutes les couches précédentes dans le bloc.\n\nDans la suite on choisira le modele qui minimisera le critère suivant '**Average Validation Loss Across Folds**' qui se réfère à la moyenne des pertes de validation sur plusieurs plis (folds) lors de l'évaluation d'un modèle d'apprentissage automatique.","metadata":{}},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self, net_version):\n        \"\"\"\n        Initialise le modèle avec EfficientNet et une couche linéaire de classification.\n\n        Parameters:\n            net_version (str): Version d'EfficientNet à utiliser.\n        \"\"\"\n        super(Net, self).__init__()\n\n        # Crée une instance d'EfficientNet pré-entraînée avec la version spécifiée\n        self.eff = EfficientNet.from_pretrained('efficientnet-' + net_version)\n\n        # Nombre de classes de sortie (modifiable en fonction des besoins)\n        num_classes = 3\n        \n        # Récupère les dimensions de sortie d'EfficientNet\n        in_features = self.eff._fc.in_features\n        \n        # Ajuste la couche linéaire pour correspondre à la sortie d'EfficientNet\n        self.out = nn.Linear(in_features, num_classes)\n\n    def forward(self, x):\n        \"\"\"\n        Méthode de propagation avant pour passer les données à travers le modèle.\n\n        Parameters:\n            x (torch.Tensor): Image en entrée.\n\n        Returns:\n            torch.Tensor: Sortie du modèle après la classification.\n        \"\"\"\n        # Passe l'image à travers EfficientNet pour extraire des caractéristiques\n        x = self.eff.extract_features(x)\n        \n        # Ajuste les dimensions de la sortie pour correspondre à la couche linéaire\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        \n        # Aplatit les dimensions de la sortie\n        x = x.view(x.size(0), -1)\n        \n        # Applique la couche linéaire pour la classification\n        return self.out(x)\n    \n    \n\nclass ResNetModel(nn.Module):\n    \n    def __init__(self):\n        super(ResNetModel, self).__init__()\n        self.resnet = models.resnet18(pretrained=True)\n        self.resnet.fc = nn.Linear(512, 3)\n\n    def forward(self, x):\n        return self.resnet(x)\n\n    \n    \nclass DenseNetModel(nn.Module):\n    \n    def __init__(self, model_type='densenet169'):\n        super(DenseNetModel, self).__init__()\n\n        if model_type == 'densenet169':\n            self.densenet = models.densenet169(pretrained=True)\n            in_features = self.densenet.classifier.in_features\n        elif model_type == 'densenet201':\n            self.densenet = models.densenet201(pretrained=True)\n            in_features = self.densenet.classifier.in_features\n        else:\n            # Par défaut, utilise DenseNet121\n            self.densenet = models.densenet121(pretrained=True)\n            in_features = self.densenet.classifier.in_features\n\n        # Geler les couches inférieures\n        for param in self.densenet.parameters():\n            param.requires_grad = False\n\n        # Ajouter une couche de classification personnalisée\n        self.densenet.classifier = nn.Sequential(\n            nn.Linear(in_features, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, 3)\n        )\n\n    def forward(self, x):\n        return self.densenet(x)\n    \n    \nclass ResNet50Model(nn.Module):\n    \n    def __init__(self):\n        super(ResNet50Model, self).__init__()\n        self.resnet50 = models.resnet50(pretrained=True)\n        self.resnet50.fc = nn.Linear(2048, 3)  # Modifiez le nombre de classes en fonction de vos besoins\n\n    def forward(self, x):\n        return self.resnet50(x)\n\n\n\n# Exemple d'utilisation de la classe Net avec des données factices\nx = torch.rand(4, 3, 224, 192)\nm = Net('b1')\noutput = m(x)\n\n# Affiche la sortie du modèle\nprint(output)\n\n# Exemple d'utilisation de la classe ResNetModel avec des données factices\nx_resnet = torch.rand(4, 3, 224, 192)\nmodel_resnet = ResNetModel()\noutput_resnet = model_resnet(x_resnet)\n\n# Affiche la sortie du modèle ResNet\nprint(output_resnet)\n\n# Exemple d'utilisation de la classe DenseNetModel avec des données factices\nx_densenet = torch.rand(4, 3, 224, 192)\nmodel_densenet = DenseNetModel()\noutput_densenet = model_densenet(x_densenet)\n\n# Affiche la sortie du modèle DenseNet\nprint(output_densenet)\n\n# Exemple d'utilisation de la classe ResNet50Model avec des données factices\nx_resnet50 = torch.rand(4, 3, 224, 192)\nmodel_resnet50 = ResNet50Model()\noutput_resnet50 = model_resnet50(x_resnet50)\n\n# Affiche la sortie du modèle ResNet-50\nprint(output_resnet50)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:08:45.184179Z","iopub.execute_input":"2023-12-10T17:08:45.184477Z","iopub.status.idle":"2023-12-10T17:08:52.587142Z","shell.execute_reply.started":"2023-12-10T17:08:45.184440Z","shell.execute_reply":"2023-12-10T17:08:52.586160Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"Loaded pretrained weights for efficientnet-b1\ntensor([[-0.0844,  0.0075, -0.0946],\n        [-0.1246, -0.0200,  0.0722],\n        [ 0.0498, -0.1421,  0.0633],\n        [ 0.0647,  0.2464, -0.0761]], grad_fn=<AddmmBackward0>)\ntensor([[ 0.2226,  1.1428,  0.0379],\n        [ 0.3751,  0.5580,  0.0209],\n        [ 0.1871,  0.7454, -0.9581],\n        [ 0.0580,  0.0209, -0.2995]], grad_fn=<AddmmBackward0>)\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/densenet169-b2777c0a.pth\" to /root/.cache/torch/hub/checkpoints/densenet169-b2777c0a.pth\n100%|██████████| 54.7M/54.7M [00:04<00:00, 13.2MB/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor([[-0.0256, -0.2064, -0.3023],\n        [ 0.2514, -0.1358,  0.1521],\n        [ 0.1718,  0.1799, -0.2455],\n        [ 0.2536, -0.3073, -0.0769]], grad_fn=<AddmmBackward0>)\ntensor([[ 0.2507,  0.1580,  0.0996],\n        [ 0.1610,  0.1490,  0.0375],\n        [ 0.3621,  0.3594,  0.5392],\n        [ 0.1712, -0.0213,  0.1217]], grad_fn=<AddmmBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Notre test a reussi avec nos différents modèles.","metadata":{}},{"cell_type":"code","source":"def check_acc(loader, model, loss_fn):\n    \"\"\"\n    Fonction pour calculer la perte moyenne sur un ensemble de données (loader).\n\n    Parameters:\n        loader (torch.utils.data.DataLoader): DataLoader pour l'ensemble de données.\n        model (torch.nn.Module): Modèle à évaluer.\n        loss_fn (torch.nn.Module): Fonction de perte à utiliser pour le calcul de la perte.\n\n    Returns:\n        torch.Tensor: La perte moyenne sur l'ensemble de données.\n    \"\"\"\n    l = 0\n    model.eval()\n    with torch.no_grad():\n        for x, y in tqdm(loader):\n            x = x.to('cuda').to(torch.float32)\n            y = y.to('cuda').to(torch.float32)\n            p = model(x)\n            l += loss_fn(p, y)\n        average_loss = l / len(loader)\n        print(average_loss)\n    model.train()\n    return average_loss\n\ndef save_checkpoint(state, filename):\n    \"\"\"\n    Fonction pour sauvegarder un checkpoint du modèle et de l'optimiseur.\n\n    Parameters:\n        state (dict): Dictionnaire contenant l'état du modèle et de l'optimiseur.\n        filename (str): Nom du fichier pour sauvegarder le checkpoint.\n    \"\"\"\n    print('--› Saving Checkpoint')\n    torch.save(state, filename)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:08:52.589729Z","iopub.execute_input":"2023-12-10T17:08:52.590031Z","iopub.status.idle":"2023-12-10T17:08:52.597792Z","shell.execute_reply.started":"2023-12-10T17:08:52.590006Z","shell.execute_reply":"2023-12-10T17:08:52.596897Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"# Fonction d'entraînement avec AMP (Automatic Mixed Precision)\ndef train_fn(loader, model, optimizer, loss_fn, scaler):\n    \"\"\"\n    Fonction d'entraînement utilisant la précision mixte (AMP).\n\n    Parameters:\n        loader (DataLoader): DataLoader contenant les données d'entraînement.\n        model (torch.nn.Module): Modèle de réseau de neurones à entraîner.\n        optimizer (torch.optim.Optimizer): Optimiseur utilisé pour mettre à jour les poids du modèle.\n        loss_fn (torch.nn.Module): Fonction de perte utilisée pour calculer la perte.\n        scaler (torch.cuda.amp.GradScaler): GradScaler d'AMP pour ajuster la précision lors de la rétropropagation.\n\n    Returns:\n        None\n    \"\"\"\n    # Itérer sur le loader avec une barre de progression\n    for x, y in tqdm(loader, desc=\"Training\"):\n        # Déplacer les données sur le GPU (s'il est disponible) et les convertir en float32\n        x = x.to('cuda').to(torch.float32)\n        y = y.to('cuda').to(torch.float32)\n\n        # Forward (calcul des prédictions et de la perte)\n        with torch.cuda.amp.autocast():\n            scores = model(x)\n            loss = loss_fn(scores, y.float())\n\n        # Backward (rétropropagation des gradients)\n        optimizer.zero_grad()\n        # Utilisation de scaler pour effectuer la rétropropagation des gradients à une précision mixte\n        scaler.scale(loss).backward()\n        # Mise à jour des poids du modèle avec l'optimiseur\n        scaler.step(optimizer)\n        # Mise à jour du scaler pour la prochaine itération\n        scaler.update()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:08:52.598965Z","iopub.execute_input":"2023-12-10T17:08:52.599256Z","iopub.status.idle":"2023-12-10T17:08:52.612232Z","shell.execute_reply.started":"2023-12-10T17:08:52.599232Z","shell.execute_reply":"2023-12-10T17:08:52.611433Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"Dans la suite, on effectue une boucle d'entraînement sur 5 plis (cross-validation) pour trois modèles différents (ResNet, EfficientNet, DenseNet). Pour chaque pli, les modèles sont entraînés et les pertes de validation moyennes sont calculées. Ensuite, les courbes des pertes de validation moyennes et des taux d'erreur sont tracées pour les trois modèles à travers les plis, permettant une comparaison visuelle de leurs performances.","metadata":{}},{"cell_type":"code","source":"# Initialiser une liste pour stocker les moyennes des pertes de validation par fold\naverage_val_losses_resnet = []\naverage_val_losses_efficientnet = []\naverage_val_losses_densenet = []\naverage_val_losses_resnet50 = []\n\n\n# Boucle d'entraînement sur 5 plis (cross-validation)\nfor fold in range(5): \n    print('----------------------------------------------- Fold: ' + str(fold))\n    \n    # Définition de la fonction de perte (Cross Entropy Loss)\n    loss_fn = nn.CrossEntropyLoss()  \n    \n    # Initialisation du scaler pour la précision mixte (Automatic Mixed Precision)\n    scaler = torch.cuda.amp.GradScaler()\n    \n    # Création d'une instance du modèle ResNet\n    model_resnet = ResNetModel().to('cuda')\n    optimizer_resnet = optim.Adam(model_resnet.parameters(), lr=LR)\n    \n    # Création d'une instance du modèle avec la version 'b1' d'EfficientNet\n    model_efficientnet = Net('b1').to('cuda')  # Renommez la variable pour éviter les confusions\n    optimizer_efficientnet = optim.Adam(model_efficientnet.parameters(), lr=LR)\n    \n    # Création d'une instance du modèle DenseNet\n    model_densenet = DenseNetModel().to('cuda')\n    optimizer_densenet = optim.Adam(model_densenet.parameters(), lr=LR)\n    \n    # Création d'une instance du modèle ResNet-50\n    model_resnet50 = ResNet50Model().to('cuda')\n    optimizer_resnet50 = optim.Adam(model_resnet50.parameters(), lr=LR)  \n\n\n    # Obtention des loaders d'entraînement et de validation pour le pli courant\n    train_loader, val_loader = get_loaders(\n        IMG, FILE, fold, H, W, BS, train_transform, val_transform\n    )\n\n    l_efficientnet = float('inf')\n    l_resnet = float('inf')\n    l_densenet = float('inf')\n    l_resnet50 = float('inf')\n    \n    # Initialiser une liste pour suivre les pertes de validation pour ce fold\n    val_losses = []\n    \n    \n    # Initialisation des variables pour l'arrêt anticipé\n    early_stopping_resnet = {'patience': 5, 'counter': 0, 'best_loss': float('inf')}\n    early_stopping_efficientnet = {'patience': 5, 'counter': 0, 'best_loss': float('inf')}\n    early_stopping_densenet = {'patience': 5, 'counter': 0, 'best_loss': float('inf')}\n    # Initialisation des variables pour l'arrêt anticipé\n    early_stopping_resnet50 = {'patience': 5, 'counter': 0, 'best_loss': float('inf')}\n\n    \n    # Boucle d'entraînement sur plusieurs époques\n    for epoch in range(NE):\n        print('----------------------------------------------- Epoch: ' + str(epoch))\n        \n        # Entraînement du modèle ResNet\n        train_fn(train_loader, model_resnet, optimizer_resnet, loss_fn, scaler)\n\n        # Entraînement du modèle EfficientNet\n        train_fn(train_loader, model_efficientnet, optimizer_efficientnet, loss_fn, scaler)\n\n        # Entraînement du modèle DenseNet\n        train_fn(train_loader, model_densenet, optimizer_densenet, loss_fn, scaler)\n        \n        # Entraînement du modèle ResNet-50\n        train_fn(train_loader, model_resnet50, optimizer_resnet50, loss_fn, scaler)\n\n        \n        # Calcul de la perte sur le loader de validation pour chaque modèle\n        val_loss_resnet = check_acc(val_loader, model_resnet, loss_fn)\n        val_loss_efficientnet = check_acc(val_loader, model_efficientnet, loss_fn)\n        val_loss_densenet = check_acc(val_loader, model_densenet, loss_fn)\n        val_loss_resnet50 = check_acc(val_loader, model_resnet50, loss_fn)\n        \n        \n        # Gestion de l'arrêt anticipé pour ResNet\n        if val_loss_resnet < early_stopping_resnet['best_loss']:\n            early_stopping_resnet['best_loss'] = val_loss_resnet\n            early_stopping_resnet['counter'] = 0\n        else:\n            early_stopping_resnet['counter'] += 1\n\n        if early_stopping_resnet['counter'] >= early_stopping_resnet['patience']:\n            print(\"Arrêt anticipé pour ResNet.\")\n            break\n\n        # Gestion de l'arrêt anticipé pour EfficientNet\n        if val_loss_efficientnet < early_stopping_efficientnet['best_loss']:\n            early_stopping_efficientnet['best_loss'] = val_loss_efficientnet\n            early_stopping_efficientnet['counter'] = 0\n        else:\n            early_stopping_efficientnet['counter'] += 1\n\n        if early_stopping_efficientnet['counter'] >= early_stopping_efficientnet['patience']:\n            print(\"Arrêt anticipé pour EfficientNet.\")\n            break\n\n        # Gestion de l'arrêt anticipé pour DenseNet\n        if val_loss_densenet < early_stopping_densenet['best_loss']:\n            early_stopping_densenet['best_loss'] = val_loss_densenet\n            early_stopping_densenet['counter'] = 0\n        else:\n            early_stopping_densenet['counter'] += 1\n\n        if early_stopping_densenet['counter'] >= early_stopping_densenet['patience']:\n            print(\"Arrêt anticipé pour DenseNet.\")\n            break\n        \n        \n        # Gestion de l'arrêt anticipé pour ResNet-50\n        if val_loss_resnet50 < early_stopping_resnet50['best_loss']:\n            early_stopping_resnet50['best_loss'] = val_loss_resnet50\n            early_stopping_resnet50['counter'] = 0\n        else:\n            early_stopping_resnet50['counter'] += 1\n\n        if early_stopping_resnet50['counter'] >= early_stopping_resnet50['patience']:\n            print(\"Arrêt anticipé pour ResNet-50.\")\n            break\n\n        \n        \n\n            \n        # Sauvegarde du checkpoint du modèle ResNet si la perte est inférieure à la meilleure perte précédente\n        if val_loss_resnet < l_resnet:\n            l_resnet = val_loss_resnet\n            checkpoint_resnet = {\n                'state_dict': model_resnet.state_dict(),\n                'optimizer': optimizer_resnet.state_dict()\n            }\n            save_checkpoint(checkpoint_resnet, filename='rice_disease_resnet' + str(fold) + '.pth.tar')\n\n        # Sauvegarde du checkpoint du modèle EfficientNet si la perte est inférieure à la meilleure perte précédente\n        if val_loss_efficientnet < l_efficientnet:\n            l_efficientnet = val_loss_efficientnet\n            checkpoint_efficientnet = {\n                'state_dict': model_efficientnet.state_dict(),\n                'optimizer': optimizer_efficientnet.state_dict()\n            }\n            save_checkpoint(checkpoint_efficientnet, filename='rice_disease_efficientnet' + str(fold) + '.pth.tar')\n\n        # Sauvegarde du checkpoint du modèle DenseNet si la perte est inférieure à la meilleure perte précédente\n        if val_loss_densenet < l_densenet:\n            l_densenet = val_loss_densenet\n            checkpoint_densenet = {\n                'state_dict': model_densenet.state_dict(),\n                'optimizer': optimizer_densenet.state_dict()\n            }\n            save_checkpoint(checkpoint_densenet, filename='rice_disease_densenet' + str(fold) + '.pth.tar')\n\n            \n            \n        # Sauvegarde du checkpoint du modèle ResNet-50 si la perte est inférieure à la meilleure perte précédente\n        if val_loss_resnet50 < l_resnet50:\n            l_resnet50 = val_loss_resnet50\n            checkpoint_resnet50 = {\n                'state_dict': model_resnet50.state_dict(),\n                'optimizer': optimizer_resnet50.state_dict()\n            }\n            save_checkpoint(checkpoint_resnet50, filename='rice_disease_resnet50' + str(fold) + '.pth.tar')\n\n    # Calculer la moyenne des pertes de validation pour ResNet-50\n    average_val_loss_resnet50 = torch.tensor([val_loss_resnet50.item()]).mean().item()\n    average_val_losses_resnet50.append(average_val_loss_resnet50)\n    \n    \n    # Calculer la moyenne des pertes de validation pour ResNet\n    average_val_loss_resnet = torch.tensor([val_loss_resnet.item()]).mean().item()\n    average_val_losses_resnet.append(average_val_loss_resnet)\n\n    # Calculer la moyenne des pertes de validation pour EfficientNet\n    average_val_loss_efficientnet = torch.tensor([val_loss_efficientnet.item()]).mean().item()\n    average_val_losses_efficientnet.append(average_val_loss_efficientnet)\n\n    # Calculer la moyenne des pertes de validation pour DenseNet\n    average_val_loss_densenet = torch.tensor([val_loss_densenet.item()]).mean().item()\n    average_val_losses_densenet.append(average_val_loss_densenet)\n\n    \n# Affichage des courbes demandées\nplt.figure(figsize=(15, 15))\n\n# Courbes pour valid_loss\nplt.subplot(2, 1, 1)\nplt.plot(average_val_losses_resnet, label='ResNet')\nplt.plot(average_val_losses_efficientnet, label='EfficientNet')\nplt.plot(average_val_losses_densenet, label='DenseNet')\nplt.plot(average_val_losses_resnet50, label='ResNet-50')\nplt.xlabel('Fold')\nplt.ylabel('Loss')\nplt.title('Average Validation Loss Across Folds')\nplt.legend()\n\n# Courbes pour error rate\nplt.subplot(2, 1, 2)\nplt.plot(1 - np.array(average_val_losses_resnet), label='ResNet')\nplt.plot(1 - np.array(average_val_losses_efficientnet), label='EfficientNet')\nplt.plot(1 - np.array(average_val_losses_densenet), label='DenseNet')\nplt.plot(1- np.array(average_val_losses_resnet50), label='ResNet-50')\nplt.xlabel('Fold')\nplt.ylabel('Error Rate')\nplt.title('Error Rate Across Folds')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:08:52.613620Z","iopub.execute_input":"2023-12-10T17:08:52.614155Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"----------------------------------------------- Fold: 0\nLoaded pretrained weights for efficientnet-b1\n----------------------------------------------- Epoch: 0\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 67/67 [00:11<00:00,  5.65it/s]\nTraining: 100%|██████████| 67/67 [00:19<00:00,  3.50it/s]\nTraining: 100%|██████████| 67/67 [00:11<00:00,  5.78it/s]\nTraining: 100%|██████████| 67/67 [00:17<00:00,  3.80it/s]\n100%|██████████| 17/17 [00:01<00:00,  8.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.4256, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17/17 [00:02<00:00,  8.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.9642, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17/17 [00:02<00:00,  7.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.8017, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17/17 [00:01<00:00,  9.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.3412, device='cuda:0')\n--› Saving Checkpoint\n--› Saving Checkpoint\n--› Saving Checkpoint\n--› Saving Checkpoint\n----------------------------------------------- Epoch: 1\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 67/67 [00:11<00:00,  5.94it/s]\nTraining: 100%|██████████| 67/67 [00:19<00:00,  3.47it/s]\nTraining: 100%|██████████| 67/67 [00:11<00:00,  5.90it/s]\nTraining: 100%|██████████| 67/67 [00:17<00:00,  3.79it/s]\n100%|██████████| 17/17 [00:02<00:00,  8.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.3409, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17/17 [00:02<00:00,  8.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.7394, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17/17 [00:02<00:00,  7.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.6990, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17/17 [00:01<00:00,  9.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.3500, device='cuda:0')\n--› Saving Checkpoint\n--› Saving Checkpoint\n--› Saving Checkpoint\n----------------------------------------------- Epoch: 2\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 67/67 [00:11<00:00,  5.92it/s]\nTraining: 100%|██████████| 67/67 [00:18<00:00,  3.58it/s]\nTraining: 100%|██████████| 67/67 [00:11<00:00,  5.86it/s]\nTraining: 100%|██████████| 67/67 [00:17<00:00,  3.82it/s]\n100%|██████████| 17/17 [00:01<00:00,  8.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.2632, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17/17 [00:01<00:00,  8.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.5208, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17/17 [00:02<00:00,  8.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.6449, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17/17 [00:01<00:00,  8.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.2538, device='cuda:0')\n--› Saving Checkpoint\n--› Saving Checkpoint\n--› Saving Checkpoint\n--› Saving Checkpoint\n----------------------------------------------- Epoch: 3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 67/67 [00:10<00:00,  6.11it/s]\nTraining: 100%|██████████| 67/67 [00:19<00:00,  3.51it/s]\nTraining: 100%|██████████| 67/67 [00:11<00:00,  5.95it/s]\nTraining: 100%|██████████| 67/67 [00:17<00:00,  3.81it/s]\n100%|██████████| 17/17 [00:01<00:00,  8.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.2321, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17/17 [00:02<00:00,  7.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.3699, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17/17 [00:02<00:00,  7.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.6045, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17/17 [00:01<00:00,  9.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.2713, device='cuda:0')\n--› Saving Checkpoint\n--› Saving Checkpoint\n--› Saving Checkpoint\n----------------------------------------------- Epoch: 4\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 67/67 [00:10<00:00,  6.43it/s]\nTraining: 100%|██████████| 67/67 [00:18<00:00,  3.56it/s]\nTraining: 100%|██████████| 67/67 [00:11<00:00,  5.88it/s]\nTraining: 100%|██████████| 67/67 [00:17<00:00,  3.81it/s]\n100%|██████████| 17/17 [00:01<00:00,  8.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.2049, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17/17 [00:02<00:00,  8.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.3099, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17/17 [00:02<00:00,  7.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.5864, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17/17 [00:01<00:00,  9.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.2299, device='cuda:0')\n--› Saving Checkpoint\n--› Saving Checkpoint\n--› Saving Checkpoint\n--› Saving Checkpoint\n----------------------------------------------- Fold: 1\nLoaded pretrained weights for efficientnet-b1\n----------------------------------------------- Epoch: 0\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 67/67 [00:10<00:00,  6.21it/s]\nTraining: 100%|██████████| 67/67 [00:18<00:00,  3.58it/s]\nTraining: 100%|██████████| 67/67 [00:11<00:00,  5.86it/s]\nTraining: 100%|██████████| 67/67 [00:17<00:00,  3.75it/s]\n100%|██████████| 17/17 [00:02<00:00,  8.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.4460, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17/17 [00:02<00:00,  8.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.8958, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17/17 [00:02<00:00,  8.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.7992, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17/17 [00:01<00:00,  9.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.4103, device='cuda:0')\n--› Saving Checkpoint\n--› Saving Checkpoint\n--› Saving Checkpoint\n--› Saving Checkpoint\n----------------------------------------------- Epoch: 1\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 67/67 [00:11<00:00,  5.95it/s]\nTraining:  10%|█         | 7/67 [00:01<00:16,  3.58it/s]","output_type":"stream"}]},{"cell_type":"markdown","source":"On observe une courbe ResNet qui est Très irrégulière ce qui ne fait pas du modèle ResNet un bon candidat dans notre cas, cependant la courbe EfficientNet semble régulière et a tendance à decroître progressivement jusqu'a atteindre son niveau minimum au 4eme fold, je decide donc de choisir le modele EfficientNet ","metadata":{}},{"cell_type":"markdown","source":"On passe maintenant à la prédiction des différentes classes des images dont les Id figurent dans les données de test ","metadata":{}},{"cell_type":"code","source":"# Charger le modèle entraîné\nmodel = DenseNetModel().to('cuda')\nmodel.load_state_dict(torch.load('/kaggle/working/rice_disease_densenet4.pth.tar')['state_dict'])\nmodel.eval()\n\n# Définir les transformations pour l'inférence\ninference_transform = transforms.Compose([\n    transforms.Resize((224, 192)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.255]),\n])\n\n# Charger le fichier Test.csv\ntest_df = pd.read_csv('/kaggle/input/dossier/Test.csv')\n\n# Créer un DataFrame pour stocker les prédictions\npredictions_df = pd.DataFrame(columns=['Image_id', 'blast', 'brown', 'healthy'])\n\n# Prédire pour chaque image dans le fichier Test.csv\nfor index, row in test_df.iterrows():\n    # Ignorer les images avec \"_rgn.jpg\" dans leur nom\n    if \"_rgn.jpg\" in row['Image_id']:\n        continue\n\n    image_path = os.path.join('/kaggle/input/hussen1/Images', row['Image_id'])\n    \n    # Charger l'image et appliquer les transformations\n    image = Image.open(image_path).convert('RGB')\n    image = inference_transform(image)\n    image = image.unsqueeze(0)\n\n    # Déplacer les images sur cuda\n    image = image.to('cuda')\n\n    # Effectuer la prédiction\n    with torch.no_grad():\n        model.eval()\n        output = model(image)\n\n    # Appliquer la fonction softmax pour obtenir les probabilités\n    probs = torch.nn.functional.softmax(output, dim=1)\n\n    # Récupérer les probabilités pour chaque classe\n    prob_blast = probs[0, 0].item()\n    prob_brown = probs[0, 1].item()\n    prob_healthy = probs[0, 2].item()\n\n    # Créer un dictionnaire avec les valeurs à ajouter\n    new_row = {\n        'Image_id': row['Image_id'],\n        'blast': prob_blast,\n        'brown': prob_brown,\n        'healthy': prob_healthy\n    }\n\n    # Ajouter la nouvelle ligne au DataFrame\n    predictions_df.loc[len(predictions_df)] = new_row\n\n# Sauvegarder les prédictions dans un fichier CSV\npredictions_df.to_csv('/kaggle/working/predictions.csv', index=False)\n\ntest_df.to_csv('/kaggle/working/SampleSubmission.csv', columns=['Image_id'], index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}